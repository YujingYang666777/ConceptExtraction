{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "M0Jc47JxMoVf"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare train data"
      ],
      "metadata": {
        "id": "P2hHrxjmcbWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train_data_path = \"/content/drive/MyDrive/fine-turning-dataset/sciNER/train.json\"\n",
        "df_train = pd.read_json(train_data_path, lines=True)\n",
        "df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "4qpYteN0caj8",
        "outputId": "4a6db2e5-ed1b-41c9-9710-0f92be54b4b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            clusters  \\\n",
              "0                             [[[17, 20], [23, 23]]]   \n",
              "1  [[[62, 64], [90, 91], [96, 98], [112, 114]], [...   \n",
              "2  [[[154, 154], [214, 214]], [[40, 44], [85, 85]...   \n",
              "3                             [[[3, 3], [110, 110]]]   \n",
              "4                             [[[35, 35], [69, 69]]]   \n",
              "\n",
              "                                           sentences  \\\n",
              "0  [[English, is, shown, to, be, trans-context-fr...   \n",
              "1  [[In, this, paper, ,, a, novel, method, to, le...   \n",
              "2  [[In, this, paper, ,, we, present, a, digital,...   \n",
              "3  [[We, propose, a, method, that, automatically,...   \n",
              "4  [[Graph, unification, remains, the, most, expe...   \n",
              "\n",
              "                                                 ner  \\\n",
              "0  [[[0, 0, Material], [10, 10, OtherScientificTe...   \n",
              "1  [[[6, 6, Method], [10, 12, OtherScientificTerm...   \n",
              "2  [[[7, 13, Method], [15, 21, Method], [23, 25, ...   \n",
              "3  [[[3, 3, Generic], [7, 7, OtherScientificTerm]...   \n",
              "4  [[[0, 1, Task], [8, 10, Task]], [[16, 17, Meth...   \n",
              "\n",
              "                                           relations                  doc_key  \n",
              "0  [[], [[29, 29, 31, 32, CONJUNCTION], [48, 49, ...                 J87-1003  \n",
              "1  [[[6, 6, 10, 12, USED-FOR], [10, 12, 14, 16, U...         CVPR_2003_18_abs  \n",
              "2  [[[7, 13, 15, 21, USED-FOR], [15, 21, 23, 25, ...  INTERSPEECH_2013_31_abs  \n",
              "3  [[[3, 3, 7, 7, USED-FOR], [7, 7, 20, 23, USED-...                 I05-5008  \n",
              "4  [[[0, 1, 8, 10, PART-OF]], [[16, 17, 22, 23, P...                 C92-2068  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4c5e0ff0-c917-44de-a6d0-367ccca33c66\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clusters</th>\n",
              "      <th>sentences</th>\n",
              "      <th>ner</th>\n",
              "      <th>relations</th>\n",
              "      <th>doc_key</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[[17, 20], [23, 23]]]</td>\n",
              "      <td>[[English, is, shown, to, be, trans-context-fr...</td>\n",
              "      <td>[[[0, 0, Material], [10, 10, OtherScientificTe...</td>\n",
              "      <td>[[], [[29, 29, 31, 32, CONJUNCTION], [48, 49, ...</td>\n",
              "      <td>J87-1003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[[62, 64], [90, 91], [96, 98], [112, 114]], [...</td>\n",
              "      <td>[[In, this, paper, ,, a, novel, method, to, le...</td>\n",
              "      <td>[[[6, 6, Method], [10, 12, OtherScientificTerm...</td>\n",
              "      <td>[[[6, 6, 10, 12, USED-FOR], [10, 12, 14, 16, U...</td>\n",
              "      <td>CVPR_2003_18_abs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[[154, 154], [214, 214]], [[40, 44], [85, 85]...</td>\n",
              "      <td>[[In, this, paper, ,, we, present, a, digital,...</td>\n",
              "      <td>[[[7, 13, Method], [15, 21, Method], [23, 25, ...</td>\n",
              "      <td>[[[7, 13, 15, 21, USED-FOR], [15, 21, 23, 25, ...</td>\n",
              "      <td>INTERSPEECH_2013_31_abs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[[[3, 3], [110, 110]]]</td>\n",
              "      <td>[[We, propose, a, method, that, automatically,...</td>\n",
              "      <td>[[[3, 3, Generic], [7, 7, OtherScientificTerm]...</td>\n",
              "      <td>[[[3, 3, 7, 7, USED-FOR], [7, 7, 20, 23, USED-...</td>\n",
              "      <td>I05-5008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[[[35, 35], [69, 69]]]</td>\n",
              "      <td>[[Graph, unification, remains, the, most, expe...</td>\n",
              "      <td>[[[0, 1, Task], [8, 10, Task]], [[16, 17, Meth...</td>\n",
              "      <td>[[[0, 1, 8, 10, PART-OF]], [[16, 17, 22, 23, P...</td>\n",
              "      <td>C92-2068</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c5e0ff0-c917-44de-a6d0-367ccca33c66')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4c5e0ff0-c917-44de-a6d0-367ccca33c66 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4c5e0ff0-c917-44de-a6d0-367ccca33c66');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_sentence_list = []\n",
        "processed_entity_list = []\n",
        "for index, row in df_train.iterrows():\n",
        "  abstract = [\" \".join(i) for i in row[\"sentences\"]]\n",
        "  abstract = \" \".join(abstract)\n",
        "  abstract_list = [item for sublist in row[\"sentences\"] for item in sublist]\n",
        "  \n",
        "  for num in range(len(row[\"sentences\"])):\n",
        "    sentence = row[\"sentences\"][num]\n",
        "    entity_list = row['ner'][num]\n",
        "    if entity_list is not None and len(entity_list) != 0:\n",
        "      text = \" \".join(sentence)\n",
        "      updated_entity_list = []\n",
        "      # entity_list = entity_list.pop()\n",
        "      for i in entity_list:\n",
        "        if type(i) == list and len(i) == 3:\n",
        "          (start, end, label) = i\n",
        "          phrase = \" \".join(abstract_list[start:end+1])\n",
        "          phrase = phrase\n",
        "          if phrase in text:\n",
        "            updated_entity_list.append((phrase, label))\n",
        "      entity = [tuple(sublist)for sublist in updated_entity_list]\n",
        "      processed_sentence_list.append(text)\n",
        "\n",
        "      processed_entity_list.append(entity)\n",
        "\n",
        "print(processed_sentence_list[0])\n",
        "print(processed_entity_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Smge5ZJehBn",
        "outputId": "5441196d-409d-4964-ff87-7c2dd02c6a61"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English is shown to be trans-context-free on the basis of coordinations of the respectively type that involve strictly syntactic cross-serial agreement .\n",
            "[('English', 'Material'), ('coordinations', 'OtherScientificTerm'), ('strictly syntactic cross-serial agreement', 'OtherScientificTerm')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task_description = \"Please extract named entities from the description and label it from {[Task], [Method], [Material], [Matric], [Generic], [OtherScientificTerm]}\"\n",
        "\n",
        "f = open(\"train.txt\", \"a\")\n",
        "for i in range(len(processed_sentence_list)):\n",
        "  solution = \"\"\n",
        "  for (word, label) in processed_entity_list[i]:\n",
        "    solution += word\n",
        "    solution += \"(\" + label + \")\" + \", \"\n",
        "  prompt = task_description + \"\\n\" + \"Description: \" + processed_sentence_list[i] + \"\\n\" + \"Extract entities: \" + solution \n",
        "  f.write(prompt)\n",
        "  f.write(\"\\n\")\n",
        "  f.write(\" \")\n",
        "  f.write(\"\\n\")\n",
        "f.close()"
      ],
      "metadata": {
        "id": "29OJpHPlcq08"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare valid data"
      ],
      "metadata": {
        "id": "v7AsfZSJj_82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_data_path = \"/content/drive/MyDrive/fine-turning-dataset/sciNER/dev.json\"\n",
        "df_valid = pd.read_json(valid_data_path, lines=True)\n",
        "df_valid.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qUYo8s_HkC0B",
        "outputId": "0faa2865-de35-40d1-c1b6-3a20ef058ace"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            clusters  \\\n",
              "0  [[[6, 17], [32, 32]], [[4, 4], [55, 55], [91, ...   \n",
              "1                           [[[90, 91], [107, 107]]]   \n",
              "2  [[[32, 32], [44, 44]], [[1, 2], [11, 11]], [[1...   \n",
              "3  [[[6, 11], [21, 21], [53, 53]], [[15, 16], [69...   \n",
              "4  [[[34, 36], [99, 101]], [[3, 5], [27, 27], [48...   \n",
              "\n",
              "                                           sentences  \\\n",
              "0  [[This, paper, presents, an, algorithm, for, c...   \n",
              "1  [[Past, work, of, generating, referring, expre...   \n",
              "2  [[An, entity-oriented, approach, to, restricte...   \n",
              "3  [[This, paper, summarizes, the, formalism, of,...   \n",
              "4  [[We, present, a, text, mining, method, for, f...   \n",
              "\n",
              "                                                 ner  \\\n",
              "0  [[[4, 4, Generic], [6, 17, Task], [20, 21, Mat...   \n",
              "1  [[[4, 5, OtherScientificTerm], [12, 13, OtherS...   \n",
              "2  [[[1, 2, Method], [4, 5, Task]], [[11, 11, Gen...   \n",
              "3  [[[4, 11, Task], [6, 11, OtherScientificTerm],...   \n",
              "4  [[[3, 5, Method], [8, 9, OtherScientificTerm],...   \n",
              "\n",
              "                                           relations            doc_key  \n",
              "0  [[[4, 4, 6, 17, USED-FOR], [20, 21, 4, 4, USED...  ICCV_2003_158_abs  \n",
              "1                           [[], [], [], [], [], []]           C04-1096  \n",
              "2  [[[1, 2, 4, 5, USED-FOR]], [], [[32, 32, 37, 3...           P84-1047  \n",
              "3  [[[15, 16, 19, 19, USED-FOR]], [], [[61, 62, 5...           C88-1066  \n",
              "4  [[[3, 5, 8, 9, USED-FOR], [13, 14, 3, 5, USED-...           C04-1116  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fe0c64ac-a2f9-40af-aab1-aafbd473d9b1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clusters</th>\n",
              "      <th>sentences</th>\n",
              "      <th>ner</th>\n",
              "      <th>relations</th>\n",
              "      <th>doc_key</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[[6, 17], [32, 32]], [[4, 4], [55, 55], [91, ...</td>\n",
              "      <td>[[This, paper, presents, an, algorithm, for, c...</td>\n",
              "      <td>[[[4, 4, Generic], [6, 17, Task], [20, 21, Mat...</td>\n",
              "      <td>[[[4, 4, 6, 17, USED-FOR], [20, 21, 4, 4, USED...</td>\n",
              "      <td>ICCV_2003_158_abs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[[90, 91], [107, 107]]]</td>\n",
              "      <td>[[Past, work, of, generating, referring, expre...</td>\n",
              "      <td>[[[4, 5, OtherScientificTerm], [12, 13, OtherS...</td>\n",
              "      <td>[[], [], [], [], [], []]</td>\n",
              "      <td>C04-1096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[[32, 32], [44, 44]], [[1, 2], [11, 11]], [[1...</td>\n",
              "      <td>[[An, entity-oriented, approach, to, restricte...</td>\n",
              "      <td>[[[1, 2, Method], [4, 5, Task]], [[11, 11, Gen...</td>\n",
              "      <td>[[[1, 2, 4, 5, USED-FOR]], [], [[32, 32, 37, 3...</td>\n",
              "      <td>P84-1047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[[[6, 11], [21, 21], [53, 53]], [[15, 16], [69...</td>\n",
              "      <td>[[This, paper, summarizes, the, formalism, of,...</td>\n",
              "      <td>[[[4, 11, Task], [6, 11, OtherScientificTerm],...</td>\n",
              "      <td>[[[15, 16, 19, 19, USED-FOR]], [], [[61, 62, 5...</td>\n",
              "      <td>C88-1066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[[[34, 36], [99, 101]], [[3, 5], [27, 27], [48...</td>\n",
              "      <td>[[We, present, a, text, mining, method, for, f...</td>\n",
              "      <td>[[[3, 5, Method], [8, 9, OtherScientificTerm],...</td>\n",
              "      <td>[[[3, 5, 8, 9, USED-FOR], [13, 14, 3, 5, USED-...</td>\n",
              "      <td>C04-1116</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe0c64ac-a2f9-40af-aab1-aafbd473d9b1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fe0c64ac-a2f9-40af-aab1-aafbd473d9b1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fe0c64ac-a2f9-40af-aab1-aafbd473d9b1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_sentence_list = []\n",
        "processed_entity_list = []\n",
        "for index, row in df_valid.iterrows():\n",
        "  abstract = [\" \".join(i) for i in row[\"sentences\"]]\n",
        "  abstract = \" \".join(abstract)\n",
        "  abstract_list = [item for sublist in row[\"sentences\"] for item in sublist]\n",
        "  \n",
        "  for num in range(len(row[\"sentences\"])):\n",
        "    sentence = row[\"sentences\"][num]\n",
        "    entity_list = row['ner'][num]\n",
        "    if entity_list is not None and len(entity_list) != 0:\n",
        "      text = \" \".join(sentence)\n",
        "      updated_entity_list = []\n",
        "      # entity_list = entity_list.pop()\n",
        "      for i in entity_list:\n",
        "        if type(i) == list and len(i) == 3:\n",
        "          (start, end, label) = i\n",
        "          phrase = \" \".join(abstract_list[start:end+1])\n",
        "          phrase = phrase\n",
        "          if phrase in text:\n",
        "            updated_entity_list.append((phrase, label))\n",
        "      entity = [tuple(sublist)for sublist in updated_entity_list]\n",
        "      processed_sentence_list.append(text)\n",
        "\n",
        "      processed_entity_list.append(entity)\n",
        "\n",
        "print(processed_sentence_list[0])\n",
        "print(processed_entity_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4GvHFsekM_Q",
        "outputId": "7d88d3ce-37b0-4f0c-d28d-ee461460388c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This paper presents an algorithm for computing optical flow , shape , motion , lighting , and albedo from an image sequence of a rigidly-moving Lambertian object under distant illumination .\n",
            "[('algorithm', 'Generic'), ('computing optical flow , shape , motion , lighting , and albedo', 'Task'), ('image sequence', 'Material'), ('rigidly-moving Lambertian object', 'Material'), ('distant illumination', 'OtherScientificTerm')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task_description = \"Please extract named entities from the description and label it from {[Task], [Method], [Material], [Matric], [Generic], [OtherScientificTerm]}\"\n",
        "\n",
        "f = open(\"dev.txt\", \"a\")\n",
        "for i in range(len(processed_sentence_list)):\n",
        "  solution = \"\"\n",
        "  for (word, label) in processed_entity_list[i]:\n",
        "    solution += word\n",
        "    solution += \"(\" + label + \")\" + \", \"\n",
        "  prompt = task_description + \"\\n\" + \"Description: \" + processed_sentence_list[i] + \"\\n\" + \"Extract entities: \" + solution \n",
        "  f.write(prompt)\n",
        "  f.write(\"\\n\")\n",
        "  f.write(\" \")\n",
        "  f.write(\"\\n\")\n",
        "f.close()"
      ],
      "metadata": {
        "id": "X-WfcefVkc7A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 structure"
      ],
      "metadata": {
        "id": "M0Jc47JxMoVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDQRJodDO3zD",
        "outputId": "d1b84b88-636b-4509-a7f4-33ce6a7ca964"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.2 tokenizers-0.13.2 transformers-4.27.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jXQBKAGbOgA9"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, \n",
        "        mlm=mlm,\n",
        "    )\n",
        "    return data_collator\n",
        "\n",
        "\n",
        "def train(train_file_path,\n",
        "          valid_file_path,\n",
        "          model_name,\n",
        "          output_dir,\n",
        "          overwrite_output_dir,\n",
        "          per_device_train_batch_size,\n",
        "          num_train_epochs,\n",
        "          save_steps):\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "  valid_dataset = load_dataset(valid_file_path, tokenizer)\n",
        "  data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "      \n",
        "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "  model.save_pretrained(output_dir)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "          output_dir=output_dir,\n",
        "          overwrite_output_dir=overwrite_output_dir,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          num_train_epochs=num_train_epochs,\n",
        "          eval_steps=1000,\n",
        "          evaluation_strategy=\"steps\"\n",
        "      )\n",
        "\n",
        "  trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "          eval_dataset=valid_dataset,\n",
        "  )\n",
        "      \n",
        "  trainer.train()\n",
        "  trainer.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune"
      ],
      "metadata": {
        "id": "1TfI_CKhMh4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_file_path = \"/content/train.txt\"\n",
        "valid_file_path = \"/content/dev.txt\"\n",
        "model_name = 'gpt2'\n",
        "output_dir = '/content/result'\n",
        "overwrite_output_dir = False\n",
        "per_device_train_batch_size = 8\n",
        "num_train_epochs = 150\n",
        "save_steps = 1000"
      ],
      "metadata": {
        "id": "HQIPAkLJOuo_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(\n",
        "    train_file_path=train_file_path,\n",
        "    valid_file_path=valid_file_path,\n",
        "    model_name=model_name,\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=overwrite_output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    save_steps=save_steps\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_d71xvXaOxcC",
        "outputId": "4ca944c3-3f8d-4c02-e3b1-b1e236e13608"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25500' max='25500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25500/25500 38:11, Epoch 150/150]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.431400</td>\n",
              "      <td>1.382363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.064600</td>\n",
              "      <td>1.450973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.827300</td>\n",
              "      <td>1.611768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.611000</td>\n",
              "      <td>1.824683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.432600</td>\n",
              "      <td>2.021281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.304900</td>\n",
              "      <td>2.130341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.223000</td>\n",
              "      <td>2.244610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.169600</td>\n",
              "      <td>2.308773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.133400</td>\n",
              "      <td>2.366167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.109900</td>\n",
              "      <td>2.413693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.094100</td>\n",
              "      <td>2.452522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.081900</td>\n",
              "      <td>2.488297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.071100</td>\n",
              "      <td>2.536084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.064600</td>\n",
              "      <td>2.563053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.058700</td>\n",
              "      <td>2.596672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.054200</td>\n",
              "      <td>2.605533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.051600</td>\n",
              "      <td>2.651455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.048300</td>\n",
              "      <td>2.650159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.045500</td>\n",
              "      <td>2.666091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.044200</td>\n",
              "      <td>2.682972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.042800</td>\n",
              "      <td>2.701428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.040700</td>\n",
              "      <td>2.694734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.039000</td>\n",
              "      <td>2.721979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.037900</td>\n",
              "      <td>2.740293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.037300</td>\n",
              "      <td>2.750460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.035700</td>\n",
              "      <td>2.760799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.035200</td>\n",
              "      <td>2.771186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.034800</td>\n",
              "      <td>2.791613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.033600</td>\n",
              "      <td>2.791607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.032500</td>\n",
              "      <td>2.792868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.032500</td>\n",
              "      <td>2.802100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.031900</td>\n",
              "      <td>2.807429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.031600</td>\n",
              "      <td>2.811518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.030900</td>\n",
              "      <td>2.819479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.030300</td>\n",
              "      <td>2.822500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.029800</td>\n",
              "      <td>2.823488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.029900</td>\n",
              "      <td>2.831463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.029000</td>\n",
              "      <td>2.841191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.028800</td>\n",
              "      <td>2.839453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.028400</td>\n",
              "      <td>2.847326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.028200</td>\n",
              "      <td>2.861822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.027800</td>\n",
              "      <td>2.846633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.028100</td>\n",
              "      <td>2.857679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.027100</td>\n",
              "      <td>2.855642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.027300</td>\n",
              "      <td>2.858512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.027000</td>\n",
              "      <td>2.860649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>0.026600</td>\n",
              "      <td>2.864646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.026000</td>\n",
              "      <td>2.867957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>0.026100</td>\n",
              "      <td>2.868708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.025800</td>\n",
              "      <td>2.870710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>0.025600</td>\n",
              "      <td>2.870283</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference test"
      ],
      "metadata": {
        "id": "CdC3ycHUMdyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer\n",
        "\n",
        "def load_model(model_path):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_tokenizer(tokenizer_path):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def generate_text(sequence, max_length):\n",
        "    model_path = \"/content/drive/MyDrive/ECSE552_project/gpt2_150epoch_sciERC/gpt2-150epoch-sciERC\"\n",
        "    model = load_model(model_path)\n",
        "    tokenizer = load_tokenizer(model_path)\n",
        "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
        "    final_outputs = model.generate(\n",
        "        ids,\n",
        "        do_sample=True,\n",
        "        max_length=max_length,\n",
        "        pad_token_id=model.config.eos_token_id,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "    )\n",
        "    return (tokenizer.decode(final_outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "oBvT3EhoPpDw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"\"\"\n",
        "Extract keyword from: Stable Diffusion is a deep learning, text-to-image model released by startup StabilityAI in 2022. It is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt.\n",
        "\"\"\"\n",
        "max_len = 200\n",
        "text = generate_text(sequence, max_len) \n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4-bJV3vPze0",
        "outputId": "1069d50b-0b69-4da1-c7e9-6f250ab3fc1b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extract keyword from: Stable Diffusion is a deep learning, text-to-image model released by startup StabilityAI in 2022. It is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt.\n",
            "Extract entities: Stable Diffusion(Method), deep learning(Method), text-to-image model(Method), it(Generic), tasks(Generic), inpainting(Task), outpainting(Task), \n",
            " \n",
            "Please extract named entities from the description and label it from {[Task], [Method], [Material], [Matric], [Generic], [OtherScientificTerm]}\n",
            "Description: We show that it is feasible to create a language model that combines language models with stochastic processes, and that a subset of the models trained on different types of data can be reliably obtained\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference (sciERC)"
      ],
      "metadata": {
        "id": "EpAHcNkwMXR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MuRzRq9vNHqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "test_data_path = \"/content/drive/MyDrive/fine-turning-dataset/sciNER/test.json\"\n",
        "df_test = pd.read_json(test_data_path, lines=True)"
      ],
      "metadata": {
        "id": "rWXMuduJM2-1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_sentence_list = []\n",
        "processed_entity_list = []\n",
        "for index, row in df_test.iterrows():\n",
        "  abstract = [\" \".join(i) for i in row[\"sentences\"]]\n",
        "  abstract = \" \".join(abstract)\n",
        "  abstract_list = [item for sublist in row[\"sentences\"] for item in sublist]\n",
        "  \n",
        "  for num in range(len(row[\"sentences\"])):\n",
        "    sentence = row[\"sentences\"][num]\n",
        "    entity_list = row['ner'][num]\n",
        "    if entity_list is not None and len(entity_list) != 0:\n",
        "      text = \" \".join(sentence)\n",
        "      updated_entity_list = []\n",
        "      # entity_list = entity_list.pop()\n",
        "      for i in entity_list:\n",
        "        if type(i) == list and len(i) == 3:\n",
        "          (start, end, label) = i\n",
        "          phrase = \" \".join(abstract_list[start:end+1])\n",
        "          phrase = phrase\n",
        "          if phrase in text:\n",
        "            updated_entity_list.append((phrase, label))\n",
        "      entity = [tuple(sublist)for sublist in updated_entity_list]\n",
        "      processed_sentence_list.append(text)\n",
        "\n",
        "      processed_entity_list.append(entity)\n",
        "\n",
        "print(len(processed_sentence_list))\n",
        "print(len(processed_entity_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-3Y0DBENaLg",
        "outputId": "8e45712b-f985-4c71-ca80-8b8d618a929e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "529\n",
            "529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_sentence_list[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "GUxb3YdQYivM",
        "outputId": "629fe322-27b4-4413-c078-6ffc74d8ec0a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of morphological analysis in Japanese text processing -LRB- -LSB- 1 -RSB- -LSB- 2 -RSB- -RRB- .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_entity_list[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axlw-vEtYmAQ",
        "outputId": "e92f9e00-0df8-4ac9-bd5e-b46aa7259b75"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Recognition of proper nouns', 'Task'),\n",
              " ('proper nouns', 'OtherScientificTerm'),\n",
              " ('Japanese text', 'Material'),\n",
              " ('morphological analysis', 'Task'),\n",
              " ('Japanese text processing', 'Task')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "task_description = \"Please extract named entities from the description and label it from {[Task], [Method], [Material], [Matric], [Generic], [OtherScientificTerm]}\"\n",
        "\n",
        "model_path = \"/content/result\"\n",
        "model = load_model(model_path)\n",
        "tokenizer = load_tokenizer(model_path)\n",
        "\n",
        "generation_list = []\n",
        "for index in tqdm(range(len(processed_sentence_list))):\n",
        "  sentence = processed_sentence_list[index]\n",
        "  prompt = task_description + \"\\n\" + \"Description:\" + sentence + \"\\n\" + \"Extracted Keyword:\"\n",
        "  ids = tokenizer.encode(f'{prompt}', return_tensors='pt')\n",
        "  final_outputs = model.generate(\n",
        "      ids,\n",
        "      do_sample=True,\n",
        "      max_length=100,\n",
        "      pad_token_id=model.config.eos_token_id,\n",
        "      top_k=50,\n",
        "      top_p=0.95,\n",
        "  )\n",
        "  text = (tokenizer.decode(final_outputs[0], skip_special_tokens=True))\n",
        "  generated_text = text.partition(\"Extracted Keyword:\")[2].partition(\"\\n\")[0]\n",
        "  generation_list.append(generated_text)\n"
      ],
      "metadata": {
        "id": "jnKsfXZGckLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percentile_list = pd.DataFrame(\n",
        "    {'sentence': processed_sentence_list,\n",
        "     'entity': processed_entity_list,\n",
        "     'generated text': generation_list\n",
        "    })\n",
        "\n",
        "percentile_list.to_csv(\"gpt2_scierc_50epoch.csv\", index = False)"
      ],
      "metadata": {
        "id": "JnE8sjRkZyZB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Move fine-tuned model to MyDrive"
      ],
      "metadata": {
        "id": "tzBvDpEC97RR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/result\n",
        "%cp -av /content/result/gpt2-150epoch-sciERC /content/drive/MyDrive/ECSE552_project/gpt2_150epoch_sciERC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3Ed4Xi07KGi",
        "outputId": "bcd0416d-9e2d-4f70-d743-4e4f9da3c0f7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/result\n",
            "'/content/result/gpt2-150epoch-sciERC' -> '/content/drive/MyDrive/ECSE552_project/gpt2_150epoch_sciERC/gpt2-150epoch-sciERC'\n",
            "'/content/result/gpt2-150epoch-sciERC/tokenizer_config.json' -> '/content/drive/MyDrive/ECSE552_project/gpt2_150epoch_sciERC/gpt2-150epoch-sciERC/tokenizer_config.json'\n",
            "'/content/result/gpt2-150epoch-sciERC/special_tokens_map.json' -> '/content/drive/MyDrive/ECSE552_project/gpt2_150epoch_sciERC/gpt2-150epoch-sciERC/special_tokens_map.json'\n",
            "'/content/result/gpt2-150epoch-sciERC/vocab.json' -> '/content/drive/MyDrive/ECSE552_project/gpt2_150epoch_sciERC/gpt2-150epoch-sciERC/vocab.json'\n",
            "'/content/result/gpt2-150epoch-sciERC/merges.txt' -> '/content/drive/MyDrive/ECSE552_project/gpt2_150epoch_sciERC/gpt2-150epoch-sciERC/merges.txt'\n",
            "'/content/result/gpt2-150epoch-sciERC/config.json' -> '/content/drive/MyDrive/ECSE552_project/gpt2_150epoch_sciERC/gpt2-150epoch-sciERC/config.json'\n",
            "'/content/result/gpt2-150epoch-sciERC/generation_config.json' -> '/content/drive/MyDrive/ECSE552_project/gpt2_150epoch_sciERC/gpt2-150epoch-sciERC/generation_config.json'\n",
            "'/content/result/gpt2-150epoch-sciERC/pytorch_model.bin' -> '/content/drive/MyDrive/ECSE552_project/gpt2_150epoch_sciERC/gpt2-150epoch-sciERC/pytorch_model.bin'\n",
            "'/content/result/gpt2-150epoch-sciERC/training_args.bin' -> '/content/drive/MyDrive/ECSE552_project/gpt2_150epoch_sciERC/gpt2-150epoch-sciERC/training_args.bin'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "5lzpaKY0aInx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/davidsbatista/NER-Evaluation.git\n",
        "!pip3 install scikit-learn\n",
        "!pip3 install nltk\n",
        "!pip3 install sklearn_crfsuite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUj-5gsLmpUi",
        "outputId": "88fb90dc-690e-4039-8e0c-3990c954dc1a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NER-Evaluation'...\n",
            "remote: Enumerating objects: 234, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 234 (delta 0), reused 2 (delta 0), pack-reused 230\u001b[K\n",
            "Receiving objects: 100% (234/234), 82.22 KiB | 20.55 MiB/s, done.\n",
            "Resolving deltas: 100% (117/117), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.1.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn_crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "  Downloading python_crfsuite-0.9.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (from sklearn_crfsuite) (0.8.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sklearn_crfsuite) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.9/dist-packages (from sklearn_crfsuite) (4.65.0)\n",
            "Installing collected packages: python-crfsuite, sklearn_crfsuite\n",
            "Successfully installed python-crfsuite-0.9.9 sklearn_crfsuite-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import sklearn_crfsuite\n",
        "\n",
        "from copy import deepcopy\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn_crfsuite.metrics import flat_classification_report\n",
        "\n",
        "from NER_Evaluation.ner_evaluation.ner_eval import collect_named_entities\n",
        "from NER_Evaluation.ner_evaluation.ner_eval import compute_metrics\n",
        "from NER_Evaluation.ner_evaluation.ner_eval import compute_precision_recall_wrapper\n",
        "from NER_Evaluation.ner_evaluation.ner_eval import namedtuple\n",
        "from NER_Evaluation.ner_evaluation.ner_eval import compute_precision_recall\n",
        "from NER_Evaluation.ner_evaluation.ner_eval import Evaluator"
      ],
      "metadata": {
        "id": "JEr_rl4lmri9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/gpt2_scierc_50epoch.csv\")\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "9VqLjJ68z01M",
        "outputId": "69cef7f8-3de3-40d5-c1da-1916c099af70"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              sentence  \\\n",
              "0    Recognition of proper nouns in Japanese text h...   \n",
              "1    It has also been studied in the framework of J...   \n",
              "2    Our approach to the Multi-lingual Evaluation T...   \n",
              "3    Our morphological analyzer has done all the ne...   \n",
              "4                The analyzer is called `` Amorph '' .   \n",
              "..                                                 ...   \n",
              "524  Preliminary modeling and recognition results a...   \n",
              "525  Fast algorithms for nearest neighbor -LRB- NN ...   \n",
              "526  Here we develop an approach for 1 distance tha...   \n",
              "527  We show how this can efficiently be combined w...   \n",
              "528  We rigorously establish the correctness of the...   \n",
              "\n",
              "                                                entity  \\\n",
              "0    [('Recognition of proper nouns', 'Task'), ('pr...   \n",
              "1    [('It', 'Generic'), ('Japanese information ext...   \n",
              "2    [('approach', 'Generic'), ('Multi-lingual Eval...   \n",
              "3    [('morphological analyzer', 'Method'), ('recog...   \n",
              "4    [('analyzer', 'Generic'), (\"`` Amorph ''\", 'Me...   \n",
              "..                                                 ...   \n",
              "524                          [('recognition', 'Task')]   \n",
              "525  [('Fast algorithms', 'Generic'), ('nearest nei...   \n",
              "526  [('approach', 'Generic'), ('1 distance', 'Othe...   \n",
              "527  [('this', 'Generic'), ('random-projection base...   \n",
              "528  [('LSH', 'Method'), ('it', 'Generic'), ('alter...   \n",
              "\n",
              "                                        generated text  \n",
              "0                                                 cogn  \n",
              "1     Japanese information extraction -LRB- -LSB-(T...  \n",
              "2     approach(Generic), Multi-lingual Evaluation T...  \n",
              "3     morphological analyzer(Method), recognition(T...  \n",
              "4     analyzer(Method), `` Amorphous ''(OtherScient...  \n",
              "..                                                 ...  \n",
              "524                modeling(Task), recognition(Task),   \n",
              "525      nearest neighbor -LRB- N -RRB- search(Task),   \n",
              "526   approach(Generic), 1 distance(OtherScientific...  \n",
              "527   random-projection(Method), 2 NN search(Task),...  \n",
              "528                methodology(Generic), LSH(Method),   \n",
              "\n",
              "[529 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4f98de64-fbbb-4b73-9c2a-7f3147b0a49b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>entity</th>\n",
              "      <th>generated text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Recognition of proper nouns in Japanese text h...</td>\n",
              "      <td>[('Recognition of proper nouns', 'Task'), ('pr...</td>\n",
              "      <td>cogn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It has also been studied in the framework of J...</td>\n",
              "      <td>[('It', 'Generic'), ('Japanese information ext...</td>\n",
              "      <td>Japanese information extraction -LRB- -LSB-(T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Our approach to the Multi-lingual Evaluation T...</td>\n",
              "      <td>[('approach', 'Generic'), ('Multi-lingual Eval...</td>\n",
              "      <td>approach(Generic), Multi-lingual Evaluation T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Our morphological analyzer has done all the ne...</td>\n",
              "      <td>[('morphological analyzer', 'Method'), ('recog...</td>\n",
              "      <td>morphological analyzer(Method), recognition(T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The analyzer is called `` Amorph '' .</td>\n",
              "      <td>[('analyzer', 'Generic'), (\"`` Amorph ''\", 'Me...</td>\n",
              "      <td>analyzer(Method), `` Amorphous ''(OtherScient...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>524</th>\n",
              "      <td>Preliminary modeling and recognition results a...</td>\n",
              "      <td>[('recognition', 'Task')]</td>\n",
              "      <td>modeling(Task), recognition(Task),</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>525</th>\n",
              "      <td>Fast algorithms for nearest neighbor -LRB- NN ...</td>\n",
              "      <td>[('Fast algorithms', 'Generic'), ('nearest nei...</td>\n",
              "      <td>nearest neighbor -LRB- N -RRB- search(Task),</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>526</th>\n",
              "      <td>Here we develop an approach for 1 distance tha...</td>\n",
              "      <td>[('approach', 'Generic'), ('1 distance', 'Othe...</td>\n",
              "      <td>approach(Generic), 1 distance(OtherScientific...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>527</th>\n",
              "      <td>We show how this can efficiently be combined w...</td>\n",
              "      <td>[('this', 'Generic'), ('random-projection base...</td>\n",
              "      <td>random-projection(Method), 2 NN search(Task),...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>528</th>\n",
              "      <td>We rigorously establish the correctness of the...</td>\n",
              "      <td>[('LSH', 'Method'), ('it', 'Generic'), ('alter...</td>\n",
              "      <td>methodology(Generic), LSH(Method),</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>529 rows Ã— 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f98de64-fbbb-4b73-9c2a-7f3147b0a49b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4f98de64-fbbb-4b73-9c2a-7f3147b0a49b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4f98de64-fbbb-4b73-9c2a-7f3147b0a49b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_entities = []\n",
        "correct_entities = []\n",
        "Entity = namedtuple(\"Entity\", \"e_type start_offset end_offset\")\n",
        "\n",
        "def find_index_sub_list(sl,l):\n",
        "    sll=len(sl)\n",
        "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
        "        if l[ind:ind+sll]==sl:\n",
        "            return ind,ind+sll-1\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "\n",
        "  sentence = row[\"sentence\"]\n",
        "  \n",
        "  # sublist created for each sentence\n",
        "  sub_predicted_entities = []\n",
        "  sub_correct_entities = []\n",
        "\n",
        "  # process true labeled entity list\n",
        "  for entity in processed_entity_list[index]:\n",
        "    (text, label) = entity\n",
        "    start_index, end_index = find_index_sub_list(text.split(), sentence.split())\n",
        "    sub_correct_entities.append(Entity(label, start_index, end_index))\n",
        "  \n",
        "  # process predicted entity list\n",
        "  solution_list = row['entity']\n",
        "  generated_list = row['generated text'].split(\", \")\n",
        "\n",
        "  for i in generated_list:\n",
        "    if \"(\" in i and \")\" in i:\n",
        "      entity = i.partition(\"(\")[0]\n",
        "      label = i.partition(\"(\")[2].partition(\")\")[0]\n",
        "      try:\n",
        "        start_index, end_index = find_index_sub_list(text.split(), sentence.split())\n",
        "        sub_predicted_entities.append(Entity(label, start_index, end_index))\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "  predicted_entities.append(sub_predicted_entities)\n",
        "  correct_entities.append(sub_correct_entities)\n",
        "\n",
        "print(len(predicted_entities))\n",
        "print(len(correct_entities))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8VM8KVqzheW",
        "outputId": "414eaed4-0b5b-4457-aa7a-2a0b7e619666"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "529\n",
            "529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tags = ['Task', \"Method\", \"Material\", \"EvaluationMatric\", \"OtherScientificTerm\", \"Generic\"]\n",
        "\n",
        "metrics_results = {'correct': 0, 'incorrect': 0, 'partial': 0,\n",
        "                   'missed': 0, 'spurious': 0, 'possible': 0, 'actual': 0, 'precision': 0, 'recall': 0}\n",
        "\n",
        "# overall results\n",
        "results = {'strict': deepcopy(metrics_results),\n",
        "           'ent_type': deepcopy(metrics_results),\n",
        "           'partial':deepcopy(metrics_results),\n",
        "           'exact':deepcopy(metrics_results)\n",
        "          }\n",
        "\n",
        "for true_ents, pred_ents in zip(correct_entities, predicted_entities):\n",
        "\n",
        "  # compute results for one message\n",
        "  tmp_results, tmp_agg_results = compute_metrics(true_ents, pred_ents, tags)\n",
        "\n",
        "  # aggregate overall results\n",
        "  for eval_schema in results.keys():\n",
        "    for metric in metrics_results.keys():\n",
        "        results[eval_schema][metric] += tmp_results[eval_schema][metric]\n",
        "  \n",
        "  # Calculate global precision and recall\n",
        "  results = compute_precision_recall_wrapper(results)"
      ],
      "metadata": {
        "id": "HT7ZJD56MWak"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9CJbNTSsPoF",
        "outputId": "d938c712-0436-4fc7-d327-7900da2529ed"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ent_type': {'correct': 442,\n",
              "  'incorrect': 593,\n",
              "  'partial': 0,\n",
              "  'missed': 1151,\n",
              "  'spurious': 37,\n",
              "  'possible': 2186,\n",
              "  'actual': 1072,\n",
              "  'precision': 0.4123134328358209,\n",
              "  'recall': 0.20219579139981703},\n",
              " 'partial': {'correct': 1030,\n",
              "  'incorrect': 0,\n",
              "  'partial': 5,\n",
              "  'missed': 1151,\n",
              "  'spurious': 37,\n",
              "  'possible': 2186,\n",
              "  'actual': 1072,\n",
              "  'precision': 0.9631529850746269,\n",
              "  'recall': 0.472323879231473},\n",
              " 'strict': {'correct': 439,\n",
              "  'incorrect': 596,\n",
              "  'partial': 0,\n",
              "  'missed': 1151,\n",
              "  'spurious': 37,\n",
              "  'possible': 2186,\n",
              "  'actual': 1072,\n",
              "  'precision': 0.40951492537313433,\n",
              "  'recall': 0.20082342177493137},\n",
              " 'exact': {'correct': 1030,\n",
              "  'incorrect': 5,\n",
              "  'partial': 0,\n",
              "  'missed': 1151,\n",
              "  'spurious': 37,\n",
              "  'possible': 2186,\n",
              "  'actual': 1072,\n",
              "  'precision': 0.960820895522388,\n",
              "  'recall': 0.47118023787740165}}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}